<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://willisfusu.github.io</id>
    <title>李二先生</title>
    <updated>2021-12-19T21:55:51.654Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://willisfusu.github.io"/>
    <link rel="self" href="https://willisfusu.github.io/atom.xml"/>
    <subtitle>&lt;section class=&quot;tagline&quot;&gt;
    Hey There, I&apos;m &lt;view class=&quot;under&quot;&gt;&lt;text class=&quot;textCon&quot;&gt;WEI LI,&lt;/text&gt;&lt;text class=&quot;borderText-lg&quot;&gt;&lt;/text&gt;&lt;/view&gt;
&lt;/section&gt;&lt;br/&gt;
&lt;section class=&quot;desc&quot;&gt;based in Manchester, UK.&lt;/section&gt;&lt;br/&gt;
&lt;section class=&quot;tagline&quot;&gt;A PhD Student in Materials&lt;/section&gt;
&lt;section class=&quot;desc&quot;&gt;Focus on &lt;view class=&quot;under&quot;&gt;&lt;text class=&quot;textCon&quot;&gt;Graphene and Ceramic&lt;/text&gt;&lt;text class=&quot;borderText-sm&quot;&gt;&lt;/text&gt; Composites.&lt;/view&gt;&lt;/section&gt;&lt;br/&gt;
&lt;section class=&quot;tagline&quot;&gt;And know a little about coding.&lt;/section&gt;
&lt;section class=&quot;desc&quot;&gt;Maybe Just A Learner 🎉🤣&lt;/section&gt;</subtitle>
    <logo>https://willisfusu.github.io/images/avatar.png</logo>
    <icon>https://willisfusu.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, 李二先生</rights>
    <entry>
        <title type="html"><![CDATA[面试总结]]></title>
        <id>https://willisfusu.github.io/post/jobinterview/</id>
        <link href="https://willisfusu.github.io/post/jobinterview/">
        </link>
        <updated>2021-12-19T11:31:33.000Z</updated>
        <summary type="html"><![CDATA[<p>最近找工作的事情算是告一段落了，适时来总结一下，把这个过程中的一些所得总结下来。</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近找工作的事情算是告一段落了，适时来总结一下，把这个过程中的一些所得总结下来。</p>
<!-- more -->
<h2 id="工作机会">工作机会</h2>
<p>找工作的过程中，我投了非常多的简历（超过 30 家）刚开始还会自己记录一下，但是后来就干脆不记录了，为什么呢？因为根本收不到回复，就单单记录自己投了哪家公司就没有多大的意义了。最后，我自己接到的面试中：</p>
<ol>
<li>华为、五矿、埃森哲，只此三家是我自己投的。（大概 1/10）</li>
<li>KLA、康宁、比亚迪、研一要么是同学推荐，要么是猎头推荐。（100%）</li>
</ol>
<p><strong>总结</strong>：在做一件事情之前，多和相关领域的同学聊一下，可能会节省自己很多很多的时间。不一定非得自己有了一定的了解再去聊，我自己感觉接入的越早，就会越节省时间，避免走弯路。</p>
<h2 id="面试准备">面试准备</h2>
<p>我在 2020 年 2 月份就开始和华为某一部门接触，一直到 7 月份主管面试没通过。当时我还没有意识到自己面试准备的问题，直到后来和一位博后师兄讨论起来，突发奇想。想问一下这位师兄在找工作的时候是如何给别人介绍自己的科研工作的。才算是解决我这个问题。总结来说：</p>
<ol>
<li>想明白面试官想知道什么（他们不是想详细的了解你的工作内容，真正想了解你工作内容的是你的导师。）</li>
<li>如何体现出自己对工作内容的思考（增加自己对于工作内容的思考部分，增加自己入职之后的工作计划。不要觉得太幼稚，但是思考一下，总是有益的。）</li>
</ol>
<p>在搞明白这两点之后，我就对我的展示 PPT 做了优化，扔掉了大量的对工作内容细节的描述（我之前太傻，为什么会指望一个不相关领域的人，在听了我的汇报之后能很快了解我的能力呢？而且还是和工作不相关的。），反而着重于讲述项目的<strong>背景、挑战与创新</strong>。这其实是在展示自己的科研思考能力。这种软能力是可以迁移到工作中的。</p>
<p>另外，我也会和面试官讨论工作的内容与规划。有的面试官是会给我一些建议，很好。如果没有，也没什么。毕竟对于工作内容的思考不仅仅是我一个新人的事情，很多职场人也不一定就有多清楚的认识。</p>
<p><strong>总结</strong>：要与别人交流，学习别人的好方法，改进自己落后的方法。</p>
<h2 id="心法">心法</h2>
<p>这一个小节取名「心法」其实是有些过的，毕竟还没有达到这个高度。我想说什么呢，</p>
<ol>
<li>做一件事，虽然有很多的挫折，但是要保持一种积极的方向。</li>
<li>真正的放下自己心中之前的固有认识。</li>
</ol>
<p>比如在找工作过程中，我刚开始对猎头不太喜欢介绍自己太多的细节，一般是把我准备的简历发出去。但是到后面，我变的非常耐心地，非常真诚地去介绍自己的工作和自己心里真正的想法。让他们给我一些建议。因为我们在一定程度上是目标一致的，是统一战线上的。</p>
<p><strong>总结</strong>：情绪要积极、态度要真诚，针对事情，针对问题。</p>
<h2 id="除了成功学之外的">除了成功学之外的</h2>
<p>写了几点，越写越有点「成功学」的味道，在成功之后回头去找原因。我是讨厌成功学的，所以我想一件事能成，除了成功学总结出来的点，肯定还有其它的成分。这些成分很难介定，天时、地利、人和。本应该是无神论一员的我，时常思考无神论者如何看待「运气」。</p>
<p>面试很多时候就有这种「运气」在里面，比如面试官恰好对你很感兴趣，比如当时恰好公司有职位空缺，比如你和面试官的气场就刚好能合到一起，能聊的开心。我不能否认这种「运气」，因为这是真实物质世界的存在。</p>
<p>所以即使有一些面试，你已经准备的很好，但是最终还是失败了。不用过于怀疑、反思自己。可能就是你我无法控制的那一部分出了问题。<strong>而我们能做的就是把我们自己可以控制的部分做好</strong>。</p>
<!-- more -->
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[黑苹果使用 M.2 NGFF 转接卡造成 Airdrop 反应慢，信号不好问题的解决方法]]></title>
        <id>https://willisfusu.github.io/post/hackintosh-ngff-card/</id>
        <link href="https://willisfusu.github.io/post/hackintosh-ngff-card/">
        </link>
        <updated>2021-06-25T13:23:42.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>TL; DR：M.2 NGFF 转接卡质量都太差，造成短路。将所有接触点做绝缘处理就可以解决问题。</strong></p>
]]></summary>
        <content type="html"><![CDATA[<p><strong>TL; DR：M.2 NGFF 转接卡质量都太差，造成短路。将所有接触点做绝缘处理就可以解决问题。</strong></p>
<!-- more -->
<h2 id="1-我遇到的问题">1. 我遇到的问题</h2>
<p>因为我自己一直在用黑苹果（我的装机方案可见<a href="https://willisfusu.github.io/post/Lenovo-M910q-OpenCore-Hackintosh/">李二先生</a> ，<a href="https://github.com/willisfusu/Lenovo-M910q-Hackintosh">willisfusu/Lenovo-M910q-Hackintosh</a>）网卡的解决方案就是买了一张苹果的原装网卡，使用 M.2 NGFF 转接卡装在了我的电脑上。Wifi 网络没有问题，能够正常使用，但是在使用 Airdrop 或者 Handoff 等功能时，却时好时坏，有时候 Airdrop 需要等很长时间，又或者 Handoff 打开的都是空白的页面。这个问题困扰了我很长时间，我一直以为是「黑苹果」系统层面造成的，也见论坛上有人说是因为 USB 定制的原因。但是我仔细检查，也按照别人方法定制了 USB 并没有解决这个问题。后来，在 tonymacx86 上看到一个帖子（<a href="https://www.tonymacx86.com/threads/solved-solution-bcm94360cs2-m-2-nfgg-wifi-and-airdrop-works-but-bt-not.275112/">原贴见此处</a>，才将这个问题解决。</p>
<h2 id="2-解决方法">2. 解决方法</h2>
<p>原来问题是出在了 M.2 NGFF 转接卡上，原作者发现现在市面上的这些转接卡质量都不太行，不知道是不是模具的问题，总之安装到主板上之后，有短路的情况出现。虽然 Wifi 可以正常使用，但是 Airdrop，Handoff 却有可能不能用。Fig.1 是转接卡，网卡，以及主板上固定位置的示意图。需要做绝缘处理的位置有两处：</p>
<ol>
<li>网卡与位置 3 之间</li>
<li>位置 2 与位置 1 之间</li>
</ol>
<p>更直观的图示可以见 Fig. 2.</p>
<figure>
<center>
<img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/20210625205336.png" alt="drawing" width="50%"/></center>
 <figcaption align = "center">Fig.1 NGFF 转接卡，网卡，主板上固定位置示意图。图片来源：<a href="https://www.tonymacx86.com/threads/solved-solution-bcm94360cs2-m-2-nfgg-wifi-and-airdrop-works-but-bt-not.275112/">原贴</a></figcaption>
</figure>
<figure>
 <center><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/20210625211456.png" alt="绝缘处理示意图" /></center>
 <figcaption align = "center">Fig.2 绝缘处理位置示意图。图片来源：<a href="https://www.tonymacx86.com/threads/solved-solution-bcm94360cs2-m-2-nfgg-wifi-and-airdrop-works-but-bt-not.275112/">原贴</a></figcaption>
</figure>
<p>经过了这样处理之后，果然 Airdrop 和 Handoff 都正常了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[对于网络评论的一些思考]]></title>
        <id>https://willisfusu.github.io/post/thinking_about_comments/</id>
        <link href="https://willisfusu.github.io/post/thinking_about_comments/">
        </link>
        <updated>2021-06-06T08:58:45.000Z</updated>
        <summary type="html"><![CDATA[<p>一段时间以来，我越来越觉得网络上的评论变得更加刻薄，更加具有攻击性。有时候很简单的几个问题，也有大量的人站到不同的阵营中去，对对方大加批斗。更有甚者，频频发出「反智」言论。因为我实在没有办法克制自己看评论的冲动，为此，我卸载了微博、酷安等软件。即使如此，还有不少无法避免的地方，像是 Bilibili 的弹幕。</p>
]]></summary>
        <content type="html"><![CDATA[<p>一段时间以来，我越来越觉得网络上的评论变得更加刻薄，更加具有攻击性。有时候很简单的几个问题，也有大量的人站到不同的阵营中去，对对方大加批斗。更有甚者，频频发出「反智」言论。因为我实在没有办法克制自己看评论的冲动，为此，我卸载了微博、酷安等软件。即使如此，还有不少无法避免的地方，像是 Bilibili 的弹幕。</p>
<!-- more -->
<p>这两天华为「鸿蒙」的发布会视频让我又刷新了对于网络评论中「戾气」的认识。我自己觉得，对于一件事物，应该允许批评和质疑。但是一定是要有依据，言之有物的批评和质疑。这是任何事物的发展和进步所必须的。如果只是单纯的讨厌和不喜欢，当然也可以评论，但是不应该虚构和污蔑，只应该谈及主观情感上的感受。</p>
<p>但是是视频弹幕中的有些言论，却让我一时觉得迷惑与不解，难道这家公司和那些发弹幕的「键盘侠」们是有什么过节？是什么力量驱动着这些人「孜孜不倦」地发着大量相同内容的言论？</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/Snipaste_2021-06-03_14-16-10.png" alt="某位「键盘侠」的弹幕" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/Snipaste_2021-06-03_14-19-10.png" alt="另一位「键盘侠」的高论" loading="lazy"></figure>
<p>对于这种言论，出于对社区环境的维护，我当然是顺手举报加屏避。但是之后我一直思考这种行为背后的动因。我觉得有以下几点可能的原因：</p>
<ol>
<li>对于自己不懂的领域没有「敬畏」之心；</li>
<li>评论的事物与生活相关，但又过于「高端」；</li>
<li>叛逆心理，追求逆反的快感；</li>
<li>单纯的「反革命」分子。</li>
</ol>
<h2 id="对于自己不懂的领域没有敬畏之心">对于自己不懂的领域没有「敬畏」之心</h2>
<p>不得不承认，现代网络的发达，让信息的获得成本大大降低了。即便是自己从来不了解的领域，通过简单的搜索也可以快速的得知一些大概的信息。这其中对于复杂、高深的知识尤其如此。因为这种知识因其复杂的特性，网络上能找到的信息往往是极度概括又浅显的。这种概括浅显的描述是有利于知识普及的，可以使得想了解、学习该领域的人可以快速的对一个问题形成一个大概的认识。但同时这就给了那些「键盘侠」们一种误解，以为自己真的通过网络搜索到的「三言两语」就成了这个方面的专家，变得了不得了，自然而然又产生一种「不过如此」的轻视与傲慢。此类人物可以在各种为大众所熟知，但其背后机理又不甚普及或者明了的事物相关的文章、视频中所见到。例如最近的「核聚变」、「鸿蒙」、「火星车」相关的文章、视频的评论、弹幕中就可以轻松找到。</p>
<p>这些人表现得如此自信，大概是源自于其真正的无知。这些人即可以是相关领域的工作者，也可以是非相关领域的人员。这种人对于知识的学习是机械且充满教条主义的，认为自己学习到的知识就是这个世界运行的真理，却丝毫不会去质疑是否有例外，是否有更深层次的原因。他们的思维模式是简单没有批判的。往往是先有了一个结论，而后又通过自己可怜且有限的知识去搜索自己想要的论据，以达到证明自己结论的目的。不得不说，在这种模式下，他们大概率是会找到自己女以为的「证据」。因为我们的大脑只会看到，我们想看到的东西。</p>
<h2 id="评论的事物太过于普及但又过于高端">评论的事物太过于普及，但又过于「高端」</h2>
<p>如果一件事物太过于普及，那么讨论的人不会很多。就像今天不会有人再去讨论空气中是不是有氧气这种物质。但如果一件事情与生活相关，但又过于「高端」，那就一定是一个非常流行的话题。就是像物理、数学从常人的讨论话题中淡出，反而生物学却是民众日常的话题。因为物理、数学与日常生活相关的部分越来越少了，同时其复杂和高难度让一般民众已经无法理解，自然就不会有人去讨论。但是同样复杂且需要大量科学学习的生物学却有很多人来讨论。因为作为一种生物，人类天天都接解到自己以及周边的动物与植物，是极其的熟悉的。同时，由于生物学的复杂和难度又让普通民众无法辨别别人的言论的真伪。又有民众基础，又可以不用担心随时被别人拆穿，自然就流行起来。崔永元之流就是借此来哄骗大众以实现其不可告人的密秘 <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>。</p>
<p>对应到「鸿蒙」系统这件事情上来，就是这种情况。手机每个人都有，系统每天都在用。「键盘侠」们心想：我就是最熟悉手机系统的人。毕竟他们每天也无有其它事情可做。再加上虽然没吃过其它系统的猪肉，但是起码见过猪跑就更加强了他们的信心。我上面的两张截图中，就是这种人。</p>
<p>对付这种人，就应该拿到真正硬核的知识，不用担心其无法理解。当这种人觉得无法理解的时候，自然就不会再去讨论这个话题，并且认为这是一个真正厉害且高端的事物了。</p>
<h2 id="叛逆心理">叛逆心理</h2>
<p>我想这种人可能是占了不少比例的，但是又是不值得花费笔墨来讨论的。因为其还处于叛逆的时候，任何的解释与努力都是成效甚微的。我们应该允许且理解心智尚未成熟的行为，每个人都经过了这种时期。</p>
<h2 id="单纯的反革命分子">单纯的「反革命」分子</h2>
<p>实在是想不出用什么词会比较准确的描述这种人，想来想去，只有这个有年代感的「反革命」最贴切。但是这种人往往不会大量出现在评论中，他们更愿意自己写一篇文章，发一个视频来单纯的引导民众，污蔑他人。像是「松鼠会」「果壳网」 「回形针」等，借科普名义实行龌龊事的，这里就不展开讲了。真正出现在评论，弹幕中的大概是他们手下的喽啰们，也就是水军。所以你会看到如果在一个视频屏避了某个用户的言论，其它视频中，竟然也有他被屏避的弹幕。这些人大部分的弹幕是雷同的，且大量重复的。对于这种人，请遵守社区评论规范，点击举报。</p>
<p>好了，忍了好几天，以上的内容不免也有些偏颇，或者有「戴帽子」的嫌疑，但是作为对一种现象的思考应该还是有益处的。</p>
<p>暂时就写这么多，后面如果想到什么就再写什么进来。</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>当年此人在我校新闻学院报告时，借助于自己多年的话术与辩论技巧竟然敢和一位生物学的教授争，可见其无知、自大与狂妄。可惜的是卢教授没能像丁院士对柴静一样打破话术与陷阱。 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scrivener 方法总结 ]]></title>
        <id>https://willisfusu.github.io/post/scrivener/</id>
        <link href="https://willisfusu.github.io/post/scrivener/">
        </link>
        <updated>2021-06-04T12:16:29.000Z</updated>
        <summary type="html"><![CDATA[<p>最近在写博士论文的时候，尝试了一下卡片写作的方法。感觉对自己的进度还是有不少的帮助。因为我使用的软件是scrivener，所以我有在考虑怎么才能把 Zotero 搭配 word 的那一套工作流转到 Scrivener 上来。</p>
<p>不得不承认，Word绝对是文字编辑领域的top 1，不光体现在其本身的素质上，也体现在与其它软件的互联互通上。要想在 scrivener 上实现文献的引用，就要麻烦不少。我找到了一篇关于[此的文章]。</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近在写博士论文的时候，尝试了一下卡片写作的方法。感觉对自己的进度还是有不少的帮助。因为我使用的软件是scrivener，所以我有在考虑怎么才能把 Zotero 搭配 word 的那一套工作流转到 Scrivener 上来。</p>
<p>不得不承认，Word绝对是文字编辑领域的top 1，不光体现在其本身的素质上，也体现在与其它软件的互联互通上。要想在 scrivener 上实现文献的引用，就要麻烦不少。我找到了一篇关于[此的文章]。</p>
<!-- more -->
<p>这里就不去写过多软件设置方面的东西了，我在这里就只记录一下 compile时需要注意的几个问题，后面可能会不定期的来更新这篇文章。在 compile 的时候选择使用<a href="https://github.com/iandol/scrivomatic">scrivomatic</a>。具体的设置可以在上面的链接中找到。</p>
<h2 id="format-选择配置">Format 选择配置</h2>
<p>在选择导入的 scrivomatic 格式之后，还需要再根据自己的实际情况配置一下。其实也很简单，主要是标题格式、等级的选择。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/scri-format.png" alt="Scrivomatic格式配置" loading="lazy"></figure>
<p>需要配置的地方已经「圈」出来了：</p>
<ol>
<li>Section Type。这里需要特别注意，因为如果不人为定义 section type，那就会按默认的来，但是很可能是不正确的。所以需要根据实际情况修改 heading 或者 subheading 再或者就是简单的 section。</li>
<li>第一步完成之后，就需要给选择的 heading or subeading 选择一个样式。这就是中间的 section layouts。 点击下面的 Assign section layouts 就可以给不同的标题等级配置样式。</li>
<li>如果还需要更加详细的样式设置，那么可以点击左下角的「齿轮⚙️️」，编辑格式。</li>
</ol>
<h2 id="表格-table">表格 Table</h2>
<p>其实表格可以是Markdown中最让人讨厌的地方了，总之找来找去也没有一个完美的解决方法。好在表格如果不多的话，也能忍受。需要注意的一点是，需要按照Pandoc的语法格式来书写表格。具体的可以参考<a href="https://pandoc.org/MANUAL.html#extension-grid_tables">此处</a>。在科研论文中，大部分的时候写的应该是「三线表」，但是Pandoc默认的并不是这个格式。</p>
<p>这里着实花费了我不少的时间，终于结合 Pandoc manual 和 Pandoc github issue 页里的一个<a href="https://github.com/jgm/pandoc/issues/3275">回答</a> 解决了这个问题。具体的解决方法如下：</p>
<h3 id="官方方法">官方方法</h3>
<p>自己定义一个 reference-doc.docx 文件。 官方给的方法是 <code>pandoc -o custom-reference.docx --print-default-data-file reference.docx</code> 先导出一个 reference.docx，然后把里面预定义的 「Table」 表格样式修改为你自己需要的，然后保存。之后再转换为 docx 时，就手动指向这个修改后的模板。</p>
<h3 id="在-scrivener-中的方法">在 scrivener 中的方法</h3>
<p>因为我是使用 scrivomatic 这个脚本格式直接转成 docx 的，所以就不想每次再自己手动使用 Pandoc 去转换。仔细查看了 scrivomatic 的脚本配置文件之后，发现 scrivomatic 没有使用 Pandoc 默认的模板文件。那就直接修改 scrivomatic 应该就可以。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/ScriRefDoc.png" alt="scrivomatic脚本配置文件" loading="lazy"></figure>
<p>试了一下果然可以！</p>
<p>需要注意的是，在修改 docx 模板中的预定义表格样式的时候，<strong>样式名要取为「Table」，并且设为默认样式。</strong></p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/ScriTableSet.png" alt="表格样式设定" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[彩色配置]]></title>
        <id>https://willisfusu.github.io/post/color-sheme/</id>
        <link href="https://willisfusu.github.io/post/color-sheme/">
        </link>
        <updated>2021-04-03T07:24:48.000Z</updated>
        <content type="html"><![CDATA[<p>前几天在网上看到别人分享的一个自己的色彩配置，我觉得还不错，就提取出来。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/Snipaste_2021-04-02_11-14-25.png" alt="" loading="lazy"></figure>
<ol>
<li>#dcac04</li>
<li>#95a5ab</li>
<li>#13719e</li>
<li>#34342c</li>
<li>#faf3e2</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[iCloud 同步功能的几个问题]]></title>
        <id>https://willisfusu.github.io/post/icloud/</id>
        <link href="https://willisfusu.github.io/post/icloud/">
        </link>
        <updated>2021-01-12T15:04:13.000Z</updated>
        <summary type="html"><![CDATA[<p>其实这个问题之前在使用 iCloud 的时候根本没有感知到，直到今天从实验室回家准备下载一个软件的时候才发生。情况是这样的：<br>
我需要从 APP Store 上下载一个软件，但是 APP Store 总是「无响应」，在网上查了一下发现这个 问题也很好解决。只要登出 Apple ID，然后再次登入就可以。</p>
]]></summary>
        <content type="html"><![CDATA[<p>其实这个问题之前在使用 iCloud 的时候根本没有感知到，直到今天从实验室回家准备下载一个软件的时候才发生。情况是这样的：<br>
我需要从 APP Store 上下载一个软件，但是 APP Store 总是「无响应」，在网上查了一下发现这个 问题也很好解决。只要登出 Apple ID，然后再次登入就可以。</p>
<!-- more -->
<h2 id="但是这样就引发了另一个问题">但是这样就引发了另一个问题</h2>
<p>因为我本机上的「Documents」是同步到了 iCloud，所以当 Apple ID 登出的时候，会将「Documents」里的文件都移动到「iCloud (Archive)」这个文件夹内。当我重新登入 ID 的时候，iCloud 不会将「iCloud (Archive)」里文件给移动回来，只会重新从云端下载。</p>
<p>本来这也没什么，但是当 iCloud 重新下载完之后，我发现有一些文件仍然在云端，没有同步到本地，而且这些文件还是我必需要用到的。所以我当时就想当然的以为将文件从「iCloud (Archive)」里复制过来就可以。但是我复制之后发现，云端的文件都没有了，系统正在上传我复制过来的文件😭️……</p>
<p>然后我就等到上传完，发现还有一部分文件是没有同步到本地的（实在是不能理解为什么，明明我已经从本地复制了 🤨），于是想着那就从云端下载吧，只要点一下☁️️标志就🆗️️了。但是我等了半天，也没有反应……</p>
<p>折腾了大半个小时，经历了取消 iCloud 同步，再次同步以及 iCloud 的各种等待之后，我发现还是不行……本以为这个问题是个 bug 我没有能力解决了的时候，我发现，如果我打开一个在「云端」还没有同步下来的文件的时候，iCloud 就开始下载，但是没有速度。<strong>我发现之所以没有速度，是因为 iCloud 卡在了一个已经下载完，但不知道为什么没有结束的进程上。</strong> 于是我点了❎️，取消掉卡住的下载进程之后，竟然就可以正常下载了……</p>
<p>真是太奇葩了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ 网红之路-使用 Python 做数据调研]]></title>
        <id>https://willisfusu.github.io/post/zhihu-sosuo/</id>
        <link href="https://willisfusu.github.io/post/zhihu-sosuo/">
        </link>
        <updated>2020-05-30T15:10:13.000Z</updated>
        <summary type="html"><![CDATA[<p>这篇文章是之前老婆突发奇想，想要在一些平台上建账号，尝试「网红」之路。我寻思着既然要做内容，那怎么也得先去了解一下自己内容在这个平台上的现况吧，比如这个话题的活跃度、发文量之类的。</p>
<p>想了一下怎么去实现，找专业的调研机构是不太可行了，没有钱。那就只能自己做，然后我想了一下，觉得直接使用平台上的搜索功能，搜索自己需要的话题，然后统计文章数量与时间关系，大体上就可以得到时间变化曲线，应该也可以做个参考。</p>
]]></summary>
        <content type="html"><![CDATA[<p>这篇文章是之前老婆突发奇想，想要在一些平台上建账号，尝试「网红」之路。我寻思着既然要做内容，那怎么也得先去了解一下自己内容在这个平台上的现况吧，比如这个话题的活跃度、发文量之类的。</p>
<p>想了一下怎么去实现，找专业的调研机构是不太可行了，没有钱。那就只能自己做，然后我想了一下，觉得直接使用平台上的搜索功能，搜索自己需要的话题，然后统计文章数量与时间关系，大体上就可以得到时间变化曲线，应该也可以做个参考。</p>
<!-- more -->
<h2 id="️-1需求">🦁️ 1.需求</h2>
<p>利用平台的搜索功能，搜索自己需要的话题，并且量化之后，做出时间变化曲线。</p>
<h2 id="2思路">🐶 2.思路</h2>
<p>应该可以使用 Python 编写爬虫，然后获取某段时间内的文章，对文章的发文量、评论量、赞数进行统计做图。</p>
<ul>
<li>爬虫可以使用 requests 库</li>
<li>数据做图使用 matplotlib 库</li>
<li>数据处理 pandas 与 pymongo</li>
</ul>
<h2 id="3具体实施">🛸 3.具体实施</h2>
<h3 id="31-首先分析目标网页">3.1 首先分析目标网页</h3>
<p>其实现在几乎所有的平台都有搜索功能，我这里选知乎做个例子，其它平台都大同小异，思路总是一样的。</p>
<ol>
<li>对目标网页进行分析</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/zhihu_001.png" alt="" loading="lazy"></figure>
<p>上面的图片是知乎上搜索的页面，我随便找了个关键词就选了「python」，搜索之后，发现乱七八糟的太多了，我又选择了时间范围为「三个月内」，结果如下：</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/zhihu_002.png" alt="" loading="lazy"></figure>
<p>嗯，这样看上去就好多了，并且三个月，统计出来的数据也有一定的参考价值了。打开 Chrome 的「检查」页面。点击 network 选项卡，在左边寻找和搜索相关的数据包。会发现一个有 search 名字的包，如下所示：</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/zhihu_003.png" alt="" loading="lazy"></figure>
<p>我们点击预览看看，果然就是返回的搜索结果，并且结果是 json 格式。仔细看一下请求的地址和数据</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/zhihu_004.png" alt="" loading="lazy"></figure>
<p><em>一般而言，返回结果不可能只有一页</em> 所以我们向下滚动搜索结果页面，看看新刷新出来的结果返回什么样的数据包。向下滚动之后，不出所料果然又返回了一个新的搜索结果。</p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/zhihu_005.png" alt="" loading="lazy"></figure>
<p>仔细看下请求的地址，发现两次请求的参数有所不同。</p>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/zhihu_006.png" alt="" loading="lazy"></figure>
<p>多了两个参数：<code>search_hash_id</code>和<code>vertical_info</code></p>
<p>分析目标网页到这，我们已经得到需要的内容：</p>
<pre><code class="language-python">请求地址：https://www.zhihu.com/api/v4/search_v3?t=general&amp;q=python&amp;correction=1&amp;offset=20&amp;limit=20&amp;lc_idx=38&amp;show_all_topics=0&amp;time_zone=three_months&amp;search_hash_id=b633d8035402af9f9e431cbffc0aa46f&amp;vertical_info=0%2C0%2C0%2C0%2C0%2C0%2C0%2C0%2C0%2C0
返回结果：json 格式
</code></pre>
<h3 id="32-爬虫代码">3.2 爬虫代码</h3>
<p>其实大部分需要用到爬虫的地方，思路都差不多。我们得到请求的地址与返回的结果格式之后，就可以来编写代码了。</p>
<p>仔细分析下请求地址中的参数：</p>
<pre><code>general: 是指返回综合搜索结果
q：搜索关键词
correction：不知道是什么意思
offset:搜索返回数据的偏移量
limit：每次返回的结果数量
lc_idx:不知道什么意思
time_zone:时间范围
search_hash_id：应该是此次执行的搜索行为的id
vertical_info：不晓得
</code></pre>
<p>有了这些理解之后，我大概就知道了如何构造请求地址了，就是循环改变 offset 的数值，从而可以不断返回新的数据。每次返回20条数据的话，offset 可以取0、20、40、……之类的数据。</p>
<p><em>请求数据</em></p>
<pre><code class="language-python">pattern = re.compile(r'\\u003c/?em\\u003e')
# remove &lt;em&gt; label
def handle_url(url):
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'accept-encoding': 'gzip',
        'accept-language': 'en-GB,en;q=0.9,zh;q=0.8,zh-CN;q=0.7,zh-TW;q=0.6,ja;q=0.5',
        &quot;cache-control&quot;: &quot;max-age=0&quot;,
        'cookie': '_xsrf=a21de17a-59ee-4d29-b4b9-5c397d0917ca; _zap=cce7d96b-ecd2-4953-b958-cc0bfbe9a2e7; BAIDU_SSP_lcr=https://www.google.com/; cap_id=&quot;NGM1OGZlYTgwMWU2NGI5YjgyZGQyYzJlYTIwZDYyMTc=|1587130242|8f2a774418c56fabb1f42aaf30d7e37dbd0df1a7&quot;; capsion_ticket=&quot;2|1:0|10:1589482049|14:capsion_ticket|44:OGEzY2NkZGVjZTIzNDg2MGE1NWNkOTYwMTgxYWUzZWI=|0e51411a2863f1881abc1a57c7b1867e6b0a059464fa43e6a2e55bf97c86bc7d&quot;; d_c0=&quot;ADCb102P-hCPTr5IshFBfxUARc-mRhjx2iY=|1584472689&quot;; l_cap_id=&quot;MTI2ZDg3OWUxNmMwNDk2Y2ExYWI4ZjY5MDRjMGIyODQ=|1587130242|6c9a2dafefd22d23c9f19142a7fb4061ebc0f45f&quot;; q_c1=e14ff8f74d144c4d9b4ce781405d22ca|1589200919000|1589200919000; r_cap_id=&quot;OGIzMzU4ZmVjODFhNDZkYzg3MWZhOTdhMDliYTExNDY=|1587130242|5e08cedf3450929c6b9816ca0dfef46c1248fa24&quot;; tst=r; z_c0=&quot;2|1:0|10:1589482084|4:z_c0|92:Mi4xQWw0MEd3QUFBQUFBTUp2WFRZXzZFQ1lBQUFCZ0FsVk5aT0NxWHdDWEVhWUJXSE1BUzZuYnV6NURCSmNrNFlPNk9R|d2595436ce2f97f37b61de7c533b0415fd03bd8dc5b95c3d3199f7ab90bb5eeb&quot;; KLBRSID=5430ad6ccb1a51f38ac194049bce5dfe|1589487035|1589486377',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'none',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36',
    }
# 我试了一下，这些 headers 好像是都需要的……没有办法减少    
    resp = requests.get(url, headers=headers)
    clear_text = pattern.sub(&quot;&quot;, resp.text)
# 因为我发现有一些返回的标题中含有 &lt;em&gt; 强调标签，所以使用正则表达式，替代掉    
    return clear_text
</code></pre>
<p>我原本以为就这样轻松完成了呢，结果第二条 url 就没有办法返回数据了，但是同样的方法，第一条 url 可以返回数据，第二条就不可以。我刚开始是以为 headers 的问题，我反复尝试了不同的参数，结果还是不成，总是返回一个 error 的 json 包。</p>
<p>在这里，我大概花了一个多小时，直到我又重新思考请求参数的含义，发现<code>search_hash_id</code>可能是一个很重要的参数。既然是执行的每次搜索的 id，那是不是每次搜索都会得到一个唯一的 id， 而我在代码中使用的 id 是浏览器执行的那次搜索。我在代码中使用，会不会和自己新执行的搜索不同？</p>
<p>但是这个 id 是什么时候产生的呢？第一次搜索是不需要 id 的，之后每次搜索都会有这个参数。会不会是第一次搜索返回的结果中包含了这个参数呢？</p>
<p>于是我回头去检查了下第一次返回的结果</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/zhihu_007.png" alt="" loading="lazy"></figure>
<p>果然被我发现了，第一次返回的 json 包中含有了此次搜索操作的 id，而且我还发现 lc_idx 这个参数也是每次不同的。</p>
<p><em>思路调整</em></p>
<p>这样我调整了下思路，我需要执行一次搜索操作，从返回的结果中得到 <code>search_hash_id</code>和<code>lc_idx</code>，然后再按照规律构造请求地址。</p>
<p><em>构造请求</em></p>
<p>按照上面调整之后的思路，我这样来构造请求：</p>
<pre><code class="language-python">def make_url(searchid, lcid, offset):
    baseurl = &quot;https://api.zhihu.com/search_v3?advert_count=0&amp;correction=1&amp;&quot;
    param = {
        &quot;lc_idx&quot;: lcid,
        &quot;limit&quot;: 20,
        &quot;offset&quot;: offset,
        &quot;q&quot;: &quot;python&quot;,
        &quot;search_hash_id&quot;: searchid,
        &quot;show_all_topics&quot;: 0,
        &quot;t&quot;: &quot;general&quot;,
        &quot;time_zone&quot;: &quot;three_months&quot;,
        'vertical_info': '0,0,0,0,0,0,0,0,0,0',
    }
    url = baseurl + urlencode(param)
    return url
</code></pre>
<p>尝试了一下，果然可以正常得到结果。但是发现一个问题：<em>只能最多返回200条数据</em> 开始我以为是反爬虫，但是我在浏览器中试了下，不断向下拉，刷新结果，也只能返回200条数据，换了几个关键词，也是同样的结果。</p>
<h3 id="33-数据处理">3.3 数据处理</h3>
<p>虽然只能得到200多条数据，但是好歹也算得到了数据。数据处理就当是练手了吧</p>
<p>怎么处理呢？我需要知道每天这个话题的发文量，然后将发文量与时间做图，就能知道这个话题的热度变化。<br>
代码如下：</p>
<pre><code class="language-python">import pymongo
import time
from matplotlib import pyplot as plt
import pandas as pd

newclient=pymongo.MongoClient('mongodb://localhost:27017')
mydb=newclient['data_research']
mycol=mydb['Zhihu']

mydata=mycol.find({})

for item in mydata:
    datestamp=item['date']
    local_date=time.localtime(datestamp)
    date_fmt=time.strftime('%Y-%m-%d',local_date)
    mycol.update_one({&quot;date&quot;:datestamp},{'$set':{'date':date_fmt}})
# 因为得到的时间是时间戳，所以需要先将其转换了正常的时间格式，并更新数据库
x=mycol.aggregate([{'$group':{'_id':&quot;$date&quot;,'num':{'$sum':1}}},{'$sort':{'_id':-1}}])
# 采用聚类计算，按照时间分组，并且计算每组中的数量，然后按照时间降序排列

df=pd.DataFrame(x)
x_a=df['_id']
y_a=df['num']
plt.plot(x_a,y_a)
plt.xticks(rotation=90)
plt.show()
# 转成 dataframe 之后，方便作图
</code></pre>
<h3 id="34-最终结果">3.4 最终结果</h3>
<p>虽然因为知乎的限制，这种方式可以说是失败了，但是也算是练习了自己的技巧吧</p>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/Figure_1.png" alt="" loading="lazy"></figure>
<h2 id="4总结">😁 4.总结</h2>
<p>首先吐槽一下，现在的互联网越来越封闭了，不只是知乎一家，其它家平台也是把数据都各种限制在自己的手中。从平台的角度说，这没有什么问题，我自己家的数据嘛。但是从用户的角度，或者从互联网的角度来说，每个平台就成了一个一个的小「孤岛」，原本互联网是降低用户获取信息的成本，但是现在或许连信息也获取不到了。</p>
<p>知乎将搜索的返回结果限制在 200 条也不知道是出于什么考虑，可能是觉得不会有人看 200 条结果？但是你也没提供任何 filter 工具呀！<strong>按时间排序、赞数排序、热度排序</strong>什么也没有。这样的搜索结果就是返回了一大堆没什么用的数据……</p>
<p>事后我搜索了一下，发现竟然也有人在吐槽知乎的搜索功能。<a href="https://www.zhihu.com/question/26617244">为什么知乎的搜索功能如此之烂？</a></p>
<p>好了，不说了，总之这此数据调研没有成功 😂️😂️😂️😂️😂️😂️</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Windows软件推荐]]></title>
        <id>https://willisfusu.github.io/post/windows-software/</id>
        <link href="https://willisfusu.github.io/post/windows-software/">
        </link>
        <updated>2020-05-24T08:50:14.000Z</updated>
        <summary type="html"><![CDATA[<p>这篇文章主要是总结并且推荐一些我在 windows 平台下长期使用并且真正值得推荐的软件。</p>
]]></summary>
        <content type="html"><![CDATA[<p>这篇文章主要是总结并且推荐一些我在 windows 平台下长期使用并且真正值得推荐的软件。</p>
<!-- more -->
<h2 id="为什么有这篇文章">为什么有这篇文章</h2>
<p>最后因为疫情在家里没事做，就在「折腾」黑苹果，也在尝试日常使用 Mac OS。因为是从 Windows 平台转过去，所以免不了要对照着在 Windows 下的使用习惯安装一些软件。刚好借这个机会，将自己在 Windows 下使用的顺手软件总结，推荐一下。这些软件可谓是我的「装机必备」</p>
<h2 id="软件推荐">软件推荐</h2>
<h3 id="total-commander">Total Commander</h3>
<p>相信这个软件已经有无数人推荐过了，在我心中，这个软件就是像 Office 一样，必备！我记得第一次用这个软件的时候是在2012年，看了「善用佳软」的推荐。当时还觉得这个软件 UI 丑，操作繁琐。当时自己真的是 Too navie。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/%E6%8D%95%E8%8E%B7.PNG" alt="" loading="lazy"></figure>
<p><strong>使用时长</strong>：到今天为止，这个软件已经使用了8年了。</p>
<p><strong>特点</strong>：代替 Windows 自带的文件浏览器，实现快速的文件定位、以及文件<strong>复制、剪切、重命名、解压、预览、分色</strong>等文件管理操作。使用熟练之后，操作文件，再也不需要鼠标。软件中有大量的快捷键，这也是新手劝退的主要因素。但是相信我，使用这个软件之后那种「行云流水」一般的感觉，真的爽！</p>
<p><strong>价格</strong>：不购买可以无限期试用，如果要买的话，37欧元，有学生优惠。建议先试用一段时间，觉得好用再入手。我也是试用了6年才入手的。是我觉让我花钱花得最开心的一个软件。</p>
<h3 id="office-365家庭版">Office 365家庭版</h3>
<p>Office 365的价格现在已经相当的亲民，起码在中国的售价是相当便宜的。在英国的我就是使用的中国区 365，英国的价格实在是高。在一些软件代理商那里可以经常搞活动，一年的家庭版才269左右？可以支持6个人，每个人6台机器。平均一个人一年才50不到，其中还包含了 1 TB 的 OneDrive 空间。实在是太划算。</p>
<p>就在我买了一年家庭版之后，曼大就开始提供免费的 365 教育版。</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/20200524234418.png" alt="" loading="lazy"></figure>
<ol>
<li><strong>不要再找激活器了，因为正版的价格太低了</strong></li>
<li>某宝有提供很低价格 365的，要注意一下管理员是可以查看你的 OneDrive 的。</li>
</ol>
<p><strong>使用时长</strong> 365家庭版已经使用3年了，之前使用的是学校授权激活的 Office</p>
<p><strong>价格</strong>：搞活动的时候269~300之间可以搞定一年。</p>
<h3 id="everything">Everything</h3>
<p>Everything 是一个搜索软件，主要作用是帮助你定位你需要的文件。这个软件很小，但是功能强悍。搜索速度不要太快，配合过滤器，可以快速的找到自己想要找的文件、程序、文件夹。Win 10 下的搜索已经改进了很多了，但是相信我，去用一下这个软件，要比系统自带的强的不知道到哪里去了。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/every.png" alt="" loading="lazy"></figure>
<p><strong>使用时长</strong> 8年，当年和 TC 一起在「善用佳软」上看到的推荐文章。</p>
<p><strong>价格</strong>：免费软件</p>
<h3 id="ditto">Ditto</h3>
<p>Ditto 是一个剪切板增强软件。想一下，是不是有这样的情境：在寻找资料时，找到了一个关键内容A，我们复制到了其它地方。然后又到到了 B、C、D等内容，也都复制到了一个地方。但是我们发现另一个地方也需要之前的某个内容A，这时候如果是没有这个软件，那么你要么再打开搜索的网页，要么去刚才粘贴的地方再复制一遍。如果是这些页面，文件都开着，并且体量不大，那还好。如果是页面已经关掉了，又或者那个文件非常大，有很多页，你是不是还要花时间去找刚才的内容呢？</p>
<p>有了这个软件，你可以轻易保存自己复制粘贴的记录，使用快捷键，快速唤出，搜索再次粘贴，尤其是大量重复工作的时候。实在是太好用了。</p>
<p>这个软件的使用场景实在是太多，也不一一去举例了，软件非常小，你可以安装了去试试。关键还是免费，没有广告。就这样功能的软件，在 Mac 下也<strong>一个 Paste 竟然丧心病狂地订阅制？？？</strong> 而且完全不如 Ditto 优雅、无打扰。</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/ditto.png" alt="" loading="lazy"></figure>
<p><strong>使用时长</strong>8年</p>
<p><strong>价格</strong> 免费软件</p>
<h3 id="internet-download-manager">Internet Download Manager</h3>
<p>IDM 是一款下载辅助软件。可以调用电脑的多核多线程，加快下载速度。我用这个软件也用了很多年，还写过文件介绍如何搭配百度云使用。但是后来下载百度云并且提示403错误，才知道是百度管的更紧了。可能现在还是有办法做到，但是太折腾了。我现在就把这个软件当成一个单纯的下载工具（好像这原本就是这个软件的功能……）方便，又好用。<br>
<img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/idm.png" alt="" loading="lazy"></p>
<p><strong>使用时长</strong>4年以上，实在记不清用了多少年了</p>
<p><strong>价格</strong>：搞活动的时候99元买的终身授权</p>
<h3 id="listary-pro">Listary Pro</h3>
<p>这个软件也是我日常重度使用的软件。大概功能和 Mac 下的 spotlight 或者 Alfred 差不多。可以快捷打开软件，或者执行一些指令。并且开发者非常慷慨的规定个人可以免费使用，也就是你不需要付费。只有单位使用时，需要 Pro 授权。 个人版好像只是不能自定义外观和新建 Project （大概是这样）。非常推荐，有了这个软件，基本上桌面上不需要再放什么软件图标了，直接用 Listary 快捷打开，好用。</p>
<p><strong>使用时长</strong>大概5年得有了</p>
<p><strong>价格</strong>去年搞活动的时候41.3元入了 Pro 授权</p>
<h3 id="zotero-文献管理">Zotero 文献管理</h3>
<p>终于说到和我博士相关的软件啦（哈哈哈哈）。 Zotero 是一个文献管理软件。其实同类型的软件有很多，像是国外的 Endnote 国内的 NoteExpress。在国内读书的时候一直是用 NoteExpress，因为图书馆提供正版授权，于是也就养成了一些使用习惯。但是曼大这边只有 Endnote 我刚开始用了一段时间，实在用不惯。主要是 Endnote 只提供了文件夹一种管理方式，这种方式明显不科学，一篇文章可能有很多的方向，只使用文件夹必然会造成文件大量重复。这时候就需要有<strong>标签</strong>功能的工具，增加一个维度的管理方式。</p>
<p>Zotero刚好可以提供文件夹，标签管理。我试用了几天就中意了。并且还让我有意外的地方。Zotero提供了 Chrome 插件，下载文献更加方便了，不需要自己下载文献，再拖到软件中。只要 chrome 上点一下，文献就自动到了它该去的地方，并且已经重新命名了。</p>
<p>至于软件的使用，我之前也写过文章，配合 OneDrive、Dropbox 等可以实现多端同步。</p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/win-zotero.png" alt="" loading="lazy"></figure>
<p><strong>使用时长</strong> 3年</p>
<p><strong>价格</strong>免费软件</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[今日头条新闻评论翻译]]></title>
        <id>https://willisfusu.github.io/post/jin-ri-tou-tiao-2/</id>
        <link href="https://willisfusu.github.io/post/jin-ri-tou-tiao-2/">
        </link>
        <updated>2020-05-22T23:22:15.000Z</updated>
        <summary type="html"><![CDATA[<p>上一篇文章我说到了由于老婆博士课题的原因，需要爬取今日头条的新闻评论，并且需要翻译成英文。我把评论的获取写成了文章，可以见<a href="https://willisfusu.github.io/post/jin-ri-tou-tiao-1/">此处</a>。今天我准备把如何使用 python 将中文翻译成英文总结一下，以备参考与之后复习之用。</p>
]]></summary>
        <content type="html"><![CDATA[<p>上一篇文章我说到了由于老婆博士课题的原因，需要爬取今日头条的新闻评论，并且需要翻译成英文。我把评论的获取写成了文章，可以见<a href="https://willisfusu.github.io/post/jin-ri-tou-tiao-1/">此处</a>。今天我准备把如何使用 python 将中文翻译成英文总结一下，以备参考与之后复习之用。</p>
<!-- more -->
<h2 id="1-为什么会有这篇文章">🐶1 为什么会有这篇文章</h2>
<p>从老婆那边拿到的项目，她要求将今日头条的新闻评论翻译为英文，以为其博士课题服务。刚开始她是想着自己翻译，后来我我她大概需要多少评论翻译成英文，告诉我说大概800条…… 于是我就问她需不需要我先给她「机翻」一下，这样后期做较对要比自己翻译快很多。于是我就成功拿到了这个项目🤣🤣🤣🤣。</p>
<h2 id="️2-项目过程">🐼️2 项目过程</h2>
<h3 id="21-确定思路">2.1 确定思路</h3>
<p>因为我自己也没有「根红苗正」的 python 学习经历，学习 python 完全是为了老婆的博士课题服务。因此这里讲到的思路可能并不是正统的程序员思路，姑且看之。</p>
<ol>
<li>读取评论内容。从我们之前存入的数据库中读取评论内容。</li>
<li>找到谷歌翻译<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>的 API 接口，将评论传入。</li>
<li>处理返回的翻译结果。</li>
</ol>
<h3 id="22-谷歌翻译-api-的配置与使用-a-namegooglea">2.2 谷歌翻译 API 的配置与使用 <a name="google"></a></h3>
<p>这一部分内容建议阅读谷歌官方给出的指南，非常详细。分为了 Basic 与 Advanced 两个版本，选择任何一个指南进行都可以完成我们翻译文本的目标。</p>
<p>谷歌翻译 API 可以<a href="https://cloud.google.com/translate/docs/quickstarts">点击这里阅读</a>。简单说分为以下四步：</p>
<ol>
<li>创建 Cloud Console project，并获得 Private Key (一个 json 文件）</li>
<li>将第1步得到的 private key 添加到环境变量中</li>
<li>配置 Google Cloud SDK （此步骤又包含了许多小步骤）</li>
<li>安装 google-cloud-translate==2.0.1 python 库。</li>
</ol>
<pre><code>pip3 install google-cloud-translate==2.0.1
</code></pre>
<h3 id="23-代码实现">2.3 代码实现</h3>
<p>其实分析到这里，整个项目也差不多完成了。剩下的代码部分比较简单。可以分为两步：1.引入 google-cloud-translate 库。 2. 传入评论。</p>
<ol>
<li>引入 google-cloud-translate 库</li>
</ol>
<pre><code class="language-python">from google.cloud import translate_v2 as translate
# 引入 google.cloud 库，并重命名
translate_client=translate.Client()
# 创建translate对象
</code></pre>
<ol start="2">
<li>传入评论内容，并处理返回结果  <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></li>
</ol>
<pre><code class="language-python">def translate_comment(col):
    print('开始%s翻译'% col)
    collection = mydb[col]
    collection.update_many({}, {'$set': {'Comment_English': 'none'}})
    # 获取评论 collection, 并且增加'Comment_English' 字段。
    querry = {&quot;Comment_English&quot;: 'none'}
    comment_array = collection.find({}, {'comment_text': 1})
    print('已获取所有评论，准备开始翻译')
    for i in comment_array:
        ch_comment = emojis.decode(i['comment_text'])
        # 由于很多评论中使用了 emoji 谷歌翻译不能正确识别 emoji，所以需要使用 emoji 库将 emoji 转换为 unicode 编码。 emoji 就是 😂️😅️☺️ 这种。
        result=translate_client.translate(ch_comment,target_language='en')
        collection.update_one(querry, {'$set': {'Comment_English': result['translatedText']}})
        # 提交评论到谷歌翻译，并将返回的翻译结果更新 'Comment_English' 字段。
</code></pre>
<ol start="3">
<li>因为有多条新闻，所以有很多组评论，需要再写一个 for 循环。</li>
</ol>
<pre><code class="language-python">collectionarray = mydb.list_collection_names()
i = 1
for col in collectionarray:
    translate_comment(col)
    print('已经完成%d个库评论数据翻译' % i)
    i += 1
</code></pre>
<h3 id="24-说明">2.4 说明</h3>
<p>如果是在像 pycharm 这样的 IDE 中执行上面的程序，很大概率（之所以说很大概率是因为我只在 pycharm 中运行过）遇到报错，提示谷歌凭证配置错误。不要担心，只要你是<a href="#google">按照上面</a>指南配置，那应该不会有错。之所以出现这个报错是因为只能在 cmd 或者 terminal 中运行。到命令终端中运行刚刚写的 py 文件，看看是不是正常执行。如果还不正常，就按照上面的指南再配置一遍。</p>
<h2 id="️3-总结">🦁️3 总结</h2>
<h3 id="31-使用到库">3.1 使用到库</h3>
<ol>
<li>pymongo 数据库</li>
<li>emojis 处理评论中的 emoji，防止谷歌翻译报错</li>
<li>google.cloud 谷歌翻译库</li>
</ol>
<h3 id="32-pymongo-修改新增字段">3.2 pymongo 修改/新增字段</h3>
<p><a href="https://www.runoob.com/python3/python-mongodb-update-document.html">update/update_many/update_one</a></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>这里选择谷歌翻译是因为人在英国，并且个人感觉谷歌翻译可能会比较准确一点？ <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>再次说一下，这个项目中不一定非得使用数据库来存储，增加新字段，并更新字段浪费了我不少时间去学习🤪🤪🤪。 <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[今日头条新闻评论获取]]></title>
        <id>https://willisfusu.github.io/post/jin-ri-tou-tiao-1/</id>
        <link href="https://willisfusu.github.io/post/jin-ri-tou-tiao-1/">
        </link>
        <updated>2020-05-22T14:31:43.000Z</updated>
        <summary type="html"><![CDATA[<p>本文属于「大型网红记录片」python 文科数据分析系列。</p>
<h2 id="为什么有这篇文章">🐶为什么有这篇文章</h2>
<p>因为老婆博士专业的原因，她需要获取不少网站的新闻或者帖子的评论，并且对评论进行数据分析或者是自然语义分析（NLP）。因此从来没有接触过 python，只有 VB 二级的我自然就成了她的技术支持，为她提供 python 爬虫和数据分析业务 🤣🤣。经过一段时间学习之后，我意识到，这些需求可能在文科的数据分析中具有某种程度上的一致性，如果能够记录下每个项目，可以供他人参考，也可以提高自己对于代码的理解。</p>
]]></summary>
        <content type="html"><![CDATA[<p>本文属于「大型网红记录片」python 文科数据分析系列。</p>
<h2 id="为什么有这篇文章">🐶为什么有这篇文章</h2>
<p>因为老婆博士专业的原因，她需要获取不少网站的新闻或者帖子的评论，并且对评论进行数据分析或者是自然语义分析（NLP）。因此从来没有接触过 python，只有 VB 二级的我自然就成了她的技术支持，为她提供 python 爬虫和数据分析业务 🤣🤣。经过一段时间学习之后，我意识到，这些需求可能在文科的数据分析中具有某种程度上的一致性，如果能够记录下每个项目，可以供他人参考，也可以提高自己对于代码的理解。</p>
<!-- more -->
<p>结合我自己的学习过程，我觉得如果能够在学习一门编程语言的过程中有一个比较明确的目的，并且从一个可以执行的项目开始，可以大幅度地提高自己的学习意愿与动力。所以我觉得如果可以，尽量从一个简单易行的项目入手，而不是拿到一个事无巨细的教程，从基础开始学习。自下而上是可以打下坚实的基础，但是自上而下的学习可以提供更强大的学习动力。</p>
<p>此篇文章产生的需求为：<strong>对指定的今日头条上的新闻，获取文章下面的评论，并且将评论翻译为英文</strong>。</p>
<h2 id="项目过程">🤖项目过程</h2>
<h3 id="目标页面分析">目标页面分析</h3>
<p>我们先随意选定一条新闻，打开新闻页面。比如这条<a href="https://www.toutiao.com/a6829669065624125959/">新闻</a>。打开之后页面如下：</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/jinri_001.png" alt="" loading="lazy"></figure>
<ol>
<li>然后在页面上右键，选择「检查」</li>
</ol>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/jinri_003.png" alt="" loading="lazy"></figure>
<ol start="2">
<li>之后会打开开发者工具页面，在 Chrome 下，应该是下面这张图这样</li>
</ol>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/jinri_002.png" alt="" loading="lazy"></figure>
<ol start="3">
<li>选择 Network， 并且刷新页面</li>
</ol>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/jinri_004.png" alt="" loading="lazy"></figure>
<p>这个时候会得到这个页面所有的网络流量内容，我需要的评论内容肯定也在其中。从评论中随便找一句话，或者一个词，在开发者工具中 Control+F，可以打开搜索。搜索刚才随便找的词句。我就随意选了「转发了」，回车执行搜索后，会发现有结果出现。下一步就是点击搜索结果。点了之后，该搜索结果所在的条目，会变暗（或者变色）。</p>
<ol start="4">
<li>双击刚刚变色的条目，会打开下面的结果 <a name="json"></a></li>
</ol>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/jinri_007.png" alt="" loading="lazy"></figure>
<p>选择 Preview 可以看到预览，看结构应该是一个 Json 文件。选择 data 打开，发现果然就是需要的评论内容。接下来就是 <strong>找到请求的 url 地址以及请求的参数信息</strong>。</p>
<ol start="5">
<li>点击 headers，查找请求信息</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/jinri_005.png" alt="" loading="lazy"></figure>
<p>从这张图里，可以知道请求的 url 地址，以及请求方法是 Get 方法。继续往下拉，可以看到请求的头信息（request headers)</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/jinri_006.png" alt="" loading="lazy"></figure>
<p><strong>通过上面五步，就完成了对目标页面的分析，那得到了什么结果呢？</strong></p>
<ul>
<li>请求地址 <code>https://www.toutiao.com/article/v2/tab_comments/?aid=24&amp;app_name=toutiao-web&amp;group_id=6829669065624125959&amp;item_id=6829669065624125959&amp;offset=0&amp;count=5</code></li>
<li>请求方法 <code>Get</code></li>
<li>请求参数 Parameters ：</li>
</ul>
<pre><code class="language-javascript">aid: 24 
app_name: toutiao-web
group_id: 6829669065624125959
item_id: 6829669065624125959
/* item_id, group_id 与文章链接中的数字是相同的，应该是文章的id*/
offset: 0
count: 5
/*offset 应该是评论的偏移量，count应该是每次返回的评论数*/
</code></pre>
<h3 id="请求地址及请求参数分析">请求地址及请求参数分析</h3>
<p>通过对目标页面分析之后，得到了请求的地址及请求参数。在使用爬虫时，我们需要自己构造请求链接，所以首先得搞清楚请求链接是怎么构造的。</p>
<p>观察这个链接：<br>
https://www.toutiao.com/article/v2/tab_comments/?aid=24&amp;app_name=toutiao-web&amp;group_id=6829669065624125959&amp;item_id=6829669065624125959&amp;offset=0&amp;count=5</p>
<p>前半部分<code>https://www.toutiao.com/article/v2/tab_comments/?</code> 可以不用管，后半部分则是请求参数组合在一起。这样看起来，我们只需要在 for 循环中 offset 偏移就可以获取所有的评论。</p>
<p><strong>验证</strong> 点击下图中标志出来的图标，清空 network 标签，然后点击评论下面的 「加载更多评论」，看看会返回什么结果。</p>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/jinri_009.png" alt="" loading="lazy"></figure>
<p>跟上面一样，找到返回的评论，点击 headers，观察请求地址</p>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/willisfusu/piconly/master/image/jinri_008.png" alt="" loading="lazy"></figure>
<p>将两次请求的地址放到一起对比，更容易找到变化：</p>
<pre><code>https://www.toutiao.com/article/v2/tab_comments/?aid=24&amp;app_name=toutiao-web&amp;group_id=6829669065624125959&amp;item_id=6829669065624125959&amp;offset=0&amp;count=5

https://www.toutiao.com/article/v2/tab_comments/?aid=24&amp;app_name=toutiao-web&amp;group_id=6829669065624125959&amp;item_id=6829669065624125959&amp;offset=5&amp;count=10
</code></pre>
<h3 id="python-程序编写">🦁Python 程序编写</h3>
<p>由于此次返回的结果直接就是 Json 格式，对于结果处理是相当友好。 代码中用到的库如下：</p>
<ul>
<li><strong>requests</strong> http 请求库，用于向服务器发送请求，获得请求结果。</li>
<li><strong>pymongo</strong> 数据库，用于数据持久化 （用其它文件存储方式也可以，我这里使用 pymongo 主要是因为自己想要熟练一下这个库的使用🦁🦁🦁🦁。完全也可以用 pandas 或者 Excel 相关的库替代）</li>
<li><strong>json</strong> json 处理 因为返回的结果是 json 格式。</li>
</ul>
<ol>
<li>请求链接构造</li>
</ol>
<p>首先我们要做的是构造请求的 url 链接，根据我们上面的分析，只要在 for 循环中更新 offset 就可以了。代码示例如下：</p>
<pre><code class="language-python">def handle_comment_url(id):
    for a in range(0, 80, 20):
        # 此处使用 range 函数，产生 offset 的数值（0， 20， 40， 60……）这里的80是因为老婆只需要前50条热评。如果想得到所有的评论，可以将80换成一个很大的数值即可，例如80000。
        param_data = {
            &quot;group_id&quot;: id,
            &quot;item_id&quot;: id,
            &quot;offset&quot;: a,
            &quot;count&quot;: 20
        }
        # param_data 就是上面分析得到的请求参数
        url = &quot;https://www.toutiao.com/article/v2/tab_comments/?aid=24&amp;app_name=toutiao-web&amp;&quot; + urlencode(param_data)
        # 构造 url，使用 urlencode 将参数组合到一起，省得自己写产生错误。
        result = handle_response(url)
        if result:
            break
        # 将构造的 url给到另一个函数，处理。这里 if 条件语句的作用是在上面想要获得所有评论时，可以及时退出循环。
</code></pre>
<ol start="2">
<li>然后我们再构造结果请求函数</li>
</ol>
<pre><code class="language-python">def handle_url(url):
    header = {
        &quot;accept&quot;: &quot;text/javascript, text/html, application/xml, text/xml, */*&quot;,
        &quot;accept - encoding&quot;: &quot;gzip, deflate, br&quot;,
        &quot;accept-language&quot;: &quot;en-GB,en;q=0.9,zh;q=0.8,zh-CN;q=0.7,zh-TW;q=0.6,ja;q=0.5&quot;,
        &quot;user-agent&quot;: &quot;user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36&quot;,
        &quot;x-requested-with&quot;: &quot;XMLHttpRequest&quot;
    }
    # 增加 headers 参数可以一定程度上防止被「反爬虫」，只能说是一定程度上。「反爬虫」与「反反爬虫」之间的斗争是另一个很长的故事。
    response = requests.get(url, headers=header)
    return (response)
</code></pre>
<ol start="3">
<li>处理返回结果的函数<br>
我们现在构造一个处理返回结果的函数，并且在这个函数里，调用上面的 <strong>请求函数</strong>。</li>
</ol>
<p>另外，这一步里需要我们对请求返回的 json 数据的结构进行分析，以便我们可以找到对应的键值对。分析 json 很简单，找一个 json 解析的网页，比如<a href="https://www.json.cn/">这一个我常用的</a>。 然后将我们<a href="#json">上面得到的 json 结果</a> 复制进这个网页中，就可以在右边得到结构比较清晰的结果。</p>
<pre><code class="language-python">def handle_response(url):
    response = handle_url(url)
    # 此处调用请求函数
    response_json = json.loads(response.text)
    # 使用 json 的 loads 方法，将 json 格式的数据，转换为 dict 格式的数据，方便 python  处理。
    break_for = True
    if (len(response_json[&quot;data&quot;]) == 0):
        print(&quot;评论已经获取完毕！！&quot;)
        return break_for
    # 这里这个 if 语句用于判断评论是否请求完，如果已经请求完，那返回 True，方便上面步骤1中及时中断循环。
    else:
        for item in response_json[&quot;data&quot;]:
            comment = {}
            comment['id'] = item[&quot;comment&quot;][&quot;id&quot;]
            comment['username']=item[&quot;comment&quot;][&quot;user_name&quot;]
            comment[&quot;comment_text&quot;] = item[&quot;comment&quot;][&quot;text&quot;]
            comment[&quot;reply_count&quot;] = item[&quot;comment&quot;][&quot;reply_count&quot;]
            comment[&quot;digg_count&quot;] = item[&quot;comment&quot;][&quot;digg_count&quot;]
            comment[&quot;creat_time&quot;] = item[&quot;comment&quot;]['create_time']
            mycol.insert_one(comment)
        print(&quot;已经完成20次数据库写入&quot;)
    # 写入数据。
</code></pre>
<h3 id="说明">说明</h3>
<ol>
<li>多线程、协程的使用</li>
</ol>
<p>这个例子中，我其实尝试使用过多线程，毕竟可以大幅度缩减时间。但是今日头条对于爬虫限制的挺厉害，我有一天晚上就被限制了 ip，导致几个小时没法访问今日头条。所以后来就干脆不使用多线程了，能稳定的运行实在是优先于时间少。 也有可能是我使用方法不对，想尝试的朋友可以尝试。</p>
<ol start="2">
<li>pymongo 的使用</li>
</ol>
<p>在这个例子中使用 pymongo 仅仅是因为我想要练习使用这个工具，数据的持久化有太多方法，使用任何一种即可。其实使用 pymongo 后面还给我带来了不少不便利😂😂😂。</p>
<ol start="3">
<li>获取所有评论的方法</li>
</ol>
<p>在这个例子中，获取所有评论的方法显得有些「暴力」直接是使用一个很大的数值来代替真实的评论总数。其实这也是无奈，因为头条将对已有评论的回复也计算在评论数中，所以就算是使用真实的评论数，也得对返回的 json--&gt; data 长度进行判断。既然是这样，直接使用一个大数值代替也是一样的。</p>
<h2 id="总结">📓总结</h2>
<h3 id="技术总结">技术总结</h3>
<p>头条新闻评论的获取就到这里了。其实大部分的爬虫都是这样的思路，最重要的就是获得请求地址，以及能构造出正确的请求 url。</p>
<p>我的代码文件可以<a href="https://github.com/willisfusu/python_projects_wife/tree/master/jin_ri_tou_tiao">在这里找到</a></p>
<h3 id="部分库及方法详解">部分库及方法详解</h3>
<p>其实我自己在写的过程中，也是不断去确认一些函数、库、方法的使用详解。比如 json 的 load 与 loads 区别。所以我在这里把一些我查过的列出来，这样也方便我自己回来复习。</p>
<ul>
<li><a href="https://www.runoob.com/python/python-func-range.html">range()</a></li>
<li><a href="https://www.runoob.com/python/python-json.html">JSON</a></li>
<li><a href="https://www.cnblogs.com/bigtreei/p/10466518.html">json.loads</a></li>
</ul>
]]></content>
    </entry>
</feed>